\documentclass[notes]{beamer}          % print frame + notes
%\documentclass[notes=only]{beamer}     % only notes
%\documentclass{beamer}                 % only frames

\usecolortheme{beaver}

% Some commonly used packages
% (copied mainly from the Utrecht University theme: https://www.overleaf.com/project/5c900fa3bd9930036341116a)
\usepackage{ragged2e}  % `\justifying` text
\usepackage{booktabs}  % Tables
\usepackage{tabularx}
\usepackage{tikz}      % Diagrams
\usetikzlibrary{calc, shapes, backgrounds}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{url}       % `\url`s
\usepackage{listings}  % Code listings
\usepackage{comment}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{array}

% Mainly math commands
\newcommand{\vect}[1]{\bm{#1}}
\usepackage{amsfonts}% to get the \mathbb alphabet
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\C}{\field{C}}
\newcommand{\R}{\field{R}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

% A variable used to exclude slides from the lecture version
\newif\iffull
\fullfalse
%\fulltrue

% Bibliography
\usepackage[uniquename=init,giveninits=true,maxcitenames=1,style=authortitle-comp]{biblatex}
\bibliography{references}

\setbeamertemplate{section in toc}{\inserttocsectionnumber.~\inserttocsection}


%Information to be included in the title page:
\title{Machine learning foundations II}
\author{Mitko Veta}
\institute{Eindhoven University of Technology

Department of Biomedical Engineering}
\date{2019}



\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Learning goals}

At the end of this lecture you will:
\begin{itemize}
    \item Have a good understanding of the basic principles of machine learning (ML) and be able to apply them in the analysis of ML methods.
    \item Have a good understanding of the different evaluation measures for ML models.
    \item Able to design good experimental setups for developing ML models.
\end{itemize}
\end{frame}

\begin{frame}{Overview}
Topics covered in this lecture:
    \tableofcontents
\end{frame}

\section{Model capacity, underfitting and overfitting}

\begin{frame}
\frametitle{Model capacity, underfitting and overfitting}
Materials:
\begin{itemize}
    \item Chapter I.5.2 from \cite{deeplearning}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Linear regression}
    \begin{center}
            \includegraphics[width=0.75\textwidth]{figures/week_2/linear_regression.pdf} \\
            $\hat{y} = \hat{w_0} + \sum_{j=1}^{p} x_j \hat{w_j}$ \\
            $\hat{y} = \vect{x}^T \hat{\vect{w}}$
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Generalization}
    \begin{itemize}
        \item The central challenge in machine learning is to design an algorithm which will perform well on new data (different from the training set data).
        \item This ability is called {\bf generalization}.
        \item {\bf Training error} is the error computed on the training set.
        \iffull
        \item During the training (learning) we aim at reducing the training error.
        \item If that is the end goal, we only have an optimization problem, not a machine learning one.
        \fi
    \end{itemize}
    \begin{center}
            \iffull
            \includegraphics[width=0.3\textwidth]{figures/week_2/linear_regression_error.pdf}
            \includegraphics[width=0.3\textwidth]{figures/week_2/linear_regression_test_set.pdf}
            \else
            \includegraphics[width=0.5\textwidth]{figures/week_2/linear_regression_error.pdf}
            \includegraphics[width=0.5\textwidth]{figures/week_2/linear_regression_test_set.pdf}
            \fi
    \end{center}
\end{frame}

\iffull
\begin{frame}
\frametitle{Generalization error}
    \begin{itemize}
        \item {\bf Generalization error}, also called {\bf test error} is defined as the expected error on new, previously unseen data.
        \item Unlike in simple optimization, in machine learning our main goal is to minimize the {\bf generalization error}.
        \item Usually the generalization error is estimated by measuring the performance on a {\bf test data set} which must be independent from the training set.
    \end{itemize}
\end{frame}
\fi

\begin{frame}
\frametitle{Example: Linear regression}
 \begin{itemize}
        \item Previously, we trained the model by minimizing the training error
        $$
        \frac{1}{m^{\mbox{(train)}}}\norm{\vect{X}^{\mbox{(train)}}\hat{\vect{w}} - \vect{y}^{\mbox{(train)}}}_2^2
        $$
        \item We would like actually to minimize the test error
         $$
        \frac{1}{m^{\mbox{(test)}}}\norm{\vect{X}^{\mbox{(test)}}\hat{\vect{w}} - \vect{y}^{\mbox{(test)}}}_2^2
        $$
\end{itemize}
\begin{center}
        \includegraphics[width=0.5\textwidth]{figures/week_2/linear_regression_error.pdf}
        \includegraphics[width=0.5\textwidth]{figures/week_2/linear_regression_test_set.pdf}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Statistical learning theory}
    \begin{itemize}
        \item The {\bf statistical learning theory} provides methods to mathematically reason about the performance on the test set although we can observe only the training set.
        \item This is possible under some assumptions about the data sets
            \begin{itemize}
                \item The training and test data are generated by drawing from a probability distribution over data sets. We refer to that as {\bf data-generating process}.
                \item {\bf i.i.d. assumptions}
                    \begin{itemize}
                        \item Examples in each data sets are {\bf independent} from each other.
                        \item The training data set and the test data set are {\bf identically distributed}, i.e., drawn from the same probability distribution.
                    \end{itemize}
            \end{itemize}

    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Underfitting and overfitting}
    \begin{itemize}
        \item The factor that determines how well a machine algorithm will perform is its ability to
            \begin{enumerate}
                \item Make the training error small.
                \item Make the difference between the training and test error small.
            \end{enumerate}
        \item These two factors correspond to the two central challenges in machine learning: {\bf underfitting} and {\bf overfitting}.
        \iffull
        \item Underfitting occurs when the model is not able to produce a sufficiently small training error.
        \item Overfitting occurs when the gap between the training and test errors is too large.
        \fi
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Model capacity}
    \begin{itemize}
        \item A {\bf capacity of the model} is its ability to fit a wide variety of functions.
        \iffull
        \item Low capacity models struggle to fit the training set (underfitting).
        \item Models with high capacity have danger to overfit the training data (e.g., by ``memorizing'' training samples).
        \fi
        \item The capacity can be controlled by choosing its {\bf hypothesis space}, i.e. the set of functions from which the learning algorithm is allowed to select the solution.
        \item Example: The linear regression algorithm has the set of all linear functions as its hypothesis space.

    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Polynomial regression}
    \begin{itemize}
        \item The linear regression algorithm can be generalized to include all polynomial functions instead of just the linear ones.
        \iffull
        \item The linear regression model is then just a special case restricted to a polynomial of degree one: $\hat{y} = b + wx$.
        \fi
        \item Moving to degree two to we obtain: $\hat{y} = b + w_1 x + w_2 x^2$.
        \begin{itemize}
            \item This can be seen as adding a new feature $x^2$.
            \item In fact, we can generalize this approach to create all sorts of hypothesis spaces, e.g.: $\hat{y} = b + w_1 x + w_2 \sin{(x)} + w_3 \sqrt{x}$.
        \end{itemize}
        \item The {\bf outuput} is still a {\bf linear} function of the parameters, so in principle it can be trained in the same way as the linear regression.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Polynomial regression}
    A comparison of a linear, degree-4, and degree-12 polynomials as predictors
    \begin{center}
        \includegraphics[width=0.33\textwidth]{figures/week_2/linear_regression_error.pdf}
        \includegraphics[width=0.33\textwidth]{figures/week_2/polynomial_regression_degree_4.pdf}
        \includegraphics[width=0.33\textwidth]{figures/week_2/polynomial_regression_degree_12.pdf}
    \end{center}
    \pause
    \begin{center}
        \includegraphics[width=0.33\textwidth]{figures/week_2/linear_regression_test_set.pdf}
        \includegraphics[width=0.33\textwidth]{figures/week_2/polynomial_regression_degree_4_test_set.pdf}
        \includegraphics[width=0.33\textwidth]{figures/week_2/polynomial_regression_degree_12_test_set.pdf}
    \end{center}
\end{frame}

\iffull
\begin{frame}
\frametitle{Overfitting and underfitting in polynomial estimation}
    \begin{itemize}
        \item Models with low capacity are not up to the task.
        \item Models with high-capacity can solve a complex task, but when the capacity is too high for the concrete (training) task there is the danger of overfitting.
        \item In our example: the linear function is unable to capture the curvature so it undefits.
        \item The degree-12 predictor is capable of fitting the training data, but it also able to find infinitely many functions that pass through the same points, so it ahs high probability of overfitting.
        \item The degree-4 function is the right solution and it generalizes well on the new data.
    \end{itemize}
\end{frame}
\fi

\begin{frame}
\frametitle{Generalization and capacity}
    \begin{itemize}
        \item Simpler functions generalize more easily, but we still need to choose a sufficiently complex hypothesis (function) to obtain small training error.
        \item Typically training error decreases with the increase of the model capacity until an (asymptotic) value is reached.
        \item The generalization error is U-shaped with the capacity range split in an underfitting and an overfitting zone (see next slide).
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Generalization and capacity}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{figures/week_2/generalization_gap.pdf}
    \end{center}
\end{frame}


\begin{frame}
\frametitle{Training set size}
    \begin{itemize}
        \item Training and generalization error vary as the size of the training data set varies.
        \item Expected generalization error never increases as the size of the training set increases.
        \item Any fixed parametric model will asymptotically approach an error value that exceeds the so called Bayes error.
        \item It is possible for the model to have optimal capacity and still have a large gap between training and generalization errors.
        \item In that case the gap usually can be reduced with increasing the number of training examples.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Training set size}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{figures/week_2/training_set_size.png}
    \end{center}
    \tiny{Figure from \cite{deeplearning}}
\end{frame}


\begin{frame}
\frametitle{The No Free Lunch theorem}
    \begin{itemize}
        \item {\bf No Free Lunch Theorem} for machine learning (Wolpert, 1996): \\
        Averaged over all possible data-generating distributions every classification algorithm has the same error rate when tested on new unobserved data.
        \item In some sense, no machine algorithm is universally better than any other algorithm.
        \item An interesting, but mainly theoretical result.
        \item In practice we often have an information about the probability distributions we deal with and can tailor our algorithms to perform well with particular distributions.
    \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Regularization}
    \begin{itemize}
        \item In addition to increasing and decreasing of the hypothesis space, i.e., the capacity, we can influence the learning algorithm by \textbf{giving preference to one solution over another in the hypothesis space}.
        \item In case both functions are eligible we can define a condition to express preference about one of the functions.
        \item The unpreferred solution is chosen only if it gives significantly better performance with the training data.
        \item \textit{More on regularization in the next lecture.}
    \end{itemize}
\end{frame}

\section{Model selection}

\begin{frame}
\frametitle{Model selection}
Materials:
\begin{itemize}
    \item Chapter I.5.3 from \cite{deeplearning}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hyperparameters and validation sets}
    \begin{itemize}
        \item {\bf Hyperparameters} are settings that can be used to control the behaviour of the algorithm.
        \item In general, the hyperparameters are not modified by the learning algorithm itself.
        \item {\bf Example}: In {\bf polynomial regression} the degree of the polynomial is a {\bf capacity} hyperparameter.
        \item A setting can be chosen to be hyperparameter when it is {\bf difficult to optimize} or - more often - when its derivation from the training set {\bf can lead to overfitting}
        \begin{itemize}
            \item Example: in polynomial regression we can always fit the data better with a higher degree polynomial.
        \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Choice of training, validation, and test sets}
    \begin{itemize}
        \item The {\bf validation set} is used during training to predict the behaviour (generalization error) of the algorithm on new data, i.e., on the {test set} and to chose the hyperparameters.
        \item Ideally these two sets are disjoint.
        \item The validation set is chosen from the training data.
        \item The training data is split in two disjoint subsets.
        \iffull
        \item One subset is used to learn the parameters of the algorithm and the other is the validation set.
        \item The subset used to learn the parameters is still typically called a {\bf training set}.
        \fi
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Choice of training, validation, and test sets}
    \begin{itemize}
        \item {\bf Question}: How large should the validation set be?
        \item Since the validation set is used to determine the hyperparameters it will typically underestimate the generalization error.
        \item However, it will usually better predict the generalization error than the training set.
        \item After the completion of the hyperparameters optimization we can estimate the generalization error using the test data.
        \item In practice the testing should be done also on different test data to avoid the test data becoming "stale".
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Choice of training, validation, and test sets}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{figures/week_2/subsets.png}
    \end{center}
\end{frame}


\begin{frame}
\frametitle{Cross-validation}
    \begin{itemize}
        \item Dividing the data set into disjoint training and test sets can result in a result in a too small validation and/or test set.
        \item In such cases all data is used to estimate the generalization error.
        \item We use procedures that repeat the training and testing on different randomly chosen subsets or splits of the original data set.
        \item The most common such procedure is the {\bf k-fold cross-validation}.
    \end{itemize}
\end{frame}

\iffull
\begin{frame}
\frametitle{Cross-validation}
    \begin{itemize}
        \item The original data is partitioned into $k$ (disjoint) subsets.
        \item The average error can be estimated by taking the average over $k$ trials.
        \item In trial $i$, the $i$-th subset is used as test set and the rest as training set.
        \item Problem: no unbiased estimators of the variance of such average error exist, but there are approximations that are used in practice.
    \end{itemize}
\end{frame}
\fi

\section{Bias and variance trade-off}

\begin{frame}
\frametitle{Bias and variance trade-off}
Materials:
\begin{itemize}
    \item Chapter I.5.4 from \cite{deeplearning}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Point estimation}
    \begin{itemize}
        \item For efficient design of learning algorithms it is useful to have formal characterizations of notions like generalization, overfitting and underfitting.
        \item To this end we introduce some definitions.
        \item {\bf Point estimation} is the attempt to provide the single "best" prediction of some quantity of interest.
        \item The quantity of interest can be a single parameter, parameter vector of some model, e.g., the weights $\vect{w}$ in the linear regression model.
        \iffull
        \item It can also be a whole function, e.g., the linear function or polynomial of some degree, like in the polynomial regression.
        \fi
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Point estimation}
    \begin{itemize}
        \item Given a parameter $\vect{\theta}$ we denote its point estimate with ${\vect{\hat{\theta}}}$.
        \item As usual, let $\{\vect{x}^{(1)}, \ldots, \vect{x}^{(m)} \}$ be $m$ independent and identically distributed (i.i.d.) data points.
        \item A {\bf point estimator} or {\bf statistic} is any function of the data
        $$
        \hat{\vect{\theta}}_m = g(\vect{x}^{(1)}, \ldots, \vect{x}^{(m)}))
        $$
        \item This definition is very general. For instance, that the value returned by $g$ need not be close to the true value $\vect{\theta}$. Also $g$ might return a value which is outside the values that $\vect{\theta}$ is allowed to have.

    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Point estimation}
    \begin{itemize}
     \item Of course, a good estimator is still a function that returns values close to $\vect{\theta}$.
    \item Since the data is drawn from a random process, point estimate $\vect{\hat\theta}$ is considered to be a random variable and $\vect{\theta}$ is fixed, but unknown parameter.
    \end{itemize}
\end{frame}

\iffull
\begin{frame}
\frametitle{Function estimation}
    \begin{itemize}
         \item In {\bf function estimation}, we assume that there is a (true) function that describes the (approximate) relationship between $\vect{x}$ and $\vect{y}$
        $$\vect{y} = f(\vect{x}) + \vect{\epsilon}$$
        where $\vect{\epsilon}$ is the part of $\vect{y}$ which is not predictable from $\vect{x}$
        \item The goal is to find the {\bf function estimate} ({\bf model}) $\hat{f}$ which is a good approximation of $f$.
        \item The linear regression and polynomial regression can be seen both illustrate scenarios that can be interpreted as either estimating a parameter $\vect{w}$ or estimating a function $\hat{f}$.
    \end{itemize}
\end{frame}
\fi

\begin{frame}
\frametitle{Bias}
    \begin{itemize}
        \item A bias of an estimator $\vect{\hat{\theta}}_m$ is defined as
        $$
        \mbox{bias}(\vect{\hat{\theta}}_m) = \field{E}(\vect{\hat{\theta}}_m) - \vect{\theta}
        $$
        where the expectation is over the data and $\vect{\theta}$ is the true underlying value.
        \item An estimator $\vect{\hat{\theta}}_m$ is {\bf unbiased} if $\mbox{bias}(\vect{\hat{\theta}}_m) = 0$. Note that this implies $\field{E}(\vect{\hat{\theta}}_m) = \vect{\theta}$.
        \item $\vect{\hat{\theta}}_m$ is {\bf asymptotically unbiased} if $\lim_{m \rightarrow \infty} \mbox{bias}(\vect{\hat{\theta}}_m) = 0$ \\
        (implying $\lim_{m \rightarrow \infty} \field{E}(\vect{\hat{\theta}}_m) = \vect{\theta}$).
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bias: example}
    \begin{itemize}
        \item {\bf Example}: Consider samples $\{x^{(1)}, \ldots, x^{(m)}\}$ i.i.d distributed according to the Gaussian distribution
        $$
        p(x^{(i)};\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left ( -\frac{1}{2} \frac{x^{(i)} - \mu}{\sigma^2} \right )
        $$
        \item The {\bf sample mean} is a common estimator of the Gaussian mean parameter
        $$
        \hat{\mu}_m = \frac{1}{m} \sum_{i=1}^{m}x{(i)}
        $$
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bias: example}
    We compute the bias as expectation by substituting the Gaussian distribution in the formula
    \begin{eqnarray*}
    \mbox{bias}(\mu_m) =  & \field{E}[\mu_m] - \mu  \\
     = & \field{E} \left [   \frac{1}{m} \sum_{i=1}^{m }x^{(i)} \right ] - \mu \\
     = & \left (   \frac{1}{m} \sum_{i=1}^{m} \field{E}[x^{(i)}] \right ) - \mu \\
     = & \left (   \frac{1}{m} \sum_{i=1}^{m} \mu \right ) - \mu \\
     = & \mu - \mu = 0
    \end{eqnarray*}
    The sample mean is an unbiased estimator of Gaussian mean parameter.
\end{frame}



\begin{frame}
\frametitle{Bias: example}
    \begin{itemize}
        \item {\bf Example}: Estimators of the variance of a Gaussian distribution
        \item We compare two different estimators of the variance $\sigma^2$ parameter
        \item {\bf Sample variance}
        $$
        \hat{\sigma}^2 = \frac{1}{m}\sum_{1}^{m} \left (  x^{(i)} - \hat{\mu}_m \right )^2
        $$
        where $\hat{\mu}$ is the sample mean.
        \item We are interested in computing
        $$
        \mbox{bias}(\hat{\sigma}^2_m) = \field{E} [\hat{\sigma}^2_m] - \sigma^2
        $$
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bias: example}
\begin{itemize}
    \item First we evaluate $\field{E} [\hat{\sigma}^2_m]$:
    $$
    \field E [\hat{\sigma}^2_m] = \field{E} \left [ \frac{1}{m}\sum_{1}^{m} \left (  x^{(i)} - \hat{\mu}_m \right )^2 \right ] = \frac{m-1}{m} \sigma^2
    $$
    \item Back to the bias
    $$
    \mbox{bias}(\hat{\sigma}^2_m) = \field{E} [\hat{\sigma}^2_m] - \sigma^2  = \frac{m-1}{m} \sigma^2 - \sigma^2 = -\frac{\sigma^2}{m}
    $$
    \item Therefore the sample variance is a \textbf{biased} estimator.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bias: example}
\begin{itemize}
    \item The {\bf unbiased variance estimator} is defined as


    $$
        \tilde{\sigma}^2 = \frac{1}{m - 1}\sum_{1}^{m} \left (  x^{(i)} - \hat{\mu}_m \right )^2
    $$
    \item Indeed
    $$
    \field E [\tilde{\sigma}^2_m] = \field{E} \left [ \frac{1}{m-1}\sum_{1}^{m} \left (  x^{(i)} - \hat{\mu}_m \right )^2 \right ] = \frac{m-1}{m-1} \sigma^2 = \sigma^2
    $$
    and the bias is $0$. \\
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Variance and standard error}
    \begin{itemize}
        \item Another important feature of an estimator is its variance.
        \item The {\bf variance} of an estimator is simple its statistical variance $\mbox{Var}(\hat{\theta})$ over the training set as a random variable.
        \item Alternatively we can compute the {\bf standard error} (the square root of the variance) $\mbox{SE}(\hat{\theta)}$.
        \item The variance or the standard error provide a measure how much the estimate would vary as we resample the data independently from the underlying data generating process.
        \item We would prefer a relatively low variance of the estimator.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Variance and standard error}
    \begin{itemize}
        \item The standard error of the mean estimator is given as
        $$
        \mbox{SE}(\hat{\mu}) = \sqrt{\mbox{Var}\left[\frac{1}{m}\sum_{i=1}^m x^{(i)}\right]} = \frac{\sigma}{\sqrt{m}}
        $$
        where $\sigma$ is the true variance of the distribution, i.e., the samples $x^{(i)}$.
        \item Neither the square root of the sample variance nor the square root of the unbiased estimator of the variance give an unbiased estimate of the standard deviation.
        \item Both approaches underestimate the true standard deviation.
        \item However, for large $m$ the approximation works quite well.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Variance and standard error}
    \begin{itemize}
        \item Often the generalization error is estimated based on the sample mean of the error on the test set.
        \item The accuracy of the estimate depends on the number of the examples.
        \item From the statistical theory (central limit theorem) we know that the mean is distributed with normal distribution for which we can establish confidence intervals.
        \item For instance, the 95\% confidence interval is given by
        $$
        \left [ \hat{\mu_m} - 1.96 \mbox{SE}(\hat{\mu}_m), \hat{\mu_m} + 1.96 \mbox{SE}(\hat{\mu}_m \right ]
        $$
        \item Then we can say that algorithm A is better than algorithm B of the confidence upper bound for the error of A is less than the corresponding lower bound of B.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Trading off bias and variance to minimize  mean squared error}
    \begin{itemize}
        \item Bias and variance measure two different sources of error in an estimator.
        \item Bias measures the expected deviation with the true value of the estimator.
        \item Variance provides a measure of the deviation from the expected value of the estimator depending on the particular data sampling.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Trading off bias and variance to minimize  mean squared error}
    \begin{itemize}
        \item Often we need to make a trade-off between these two.
        \item The most common way to do this is via cross-validation.
        \item An alternative is to compare the {\bf mean squared error} (MSE) of the estimates.
        $$
        \mbox{MSE} = \field{E}[(\hat{\theta}_m -  \theta)^2] = \mbox{bias}(\hat{\theta}_m)^2 + \mbox{Var}(\hat{\theta}_m)
        $$
        \item The smaller MSE the better - so minimizing both the bias and variance is always preferable.

    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bias and variance}
    \begin{itemize}
        \item Our original goal was to provide a mathematical support for the notions of capacity, underfitting, and overfitting.
        \item Indeed there is a close relationship between these three concepts and bias and variance.
        \item When generalization error is measured by MSE (and hence indirectly via bias and variance) increasing capacity tends to increase variance and decrease bias.
        \item Again the generalization as a function of capacity is given by an U-shaped curve.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bias and variance}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{figures/week_2/bias_variance.pdf}
    \end{center}
\end{frame}

\iffull
\begin{frame}
\frametitle{Consistency}
    \begin{itemize}
        \item So far we considered fixed size of the training data sets.
        \item We expect that as the number $m$ of training examples grows the estimators will converge to the true value of the parameters.
        \item More formally this is captured in the notion of {\bf consistency}
        $$
        \mbox{plim}_{m \rightarrow \infty} \hat{\theta}_m = \theta
        $$
        where $ \mbox{plim}$ denotes convergence in probability: for any $\epsilon > 0$, $P(\mid \hat{\theta}_m - \theta \mid > \epsilon) \rightarrow 0$ as $m \rightarrow \infty$.
        \item For consistent models the bias decreases as $m$ increases, however a decreasing bias (when $m$ increases) does not imply consistency.
    \end{itemize}
\end{frame}
\fi

\section{Maximum likelihood estimation}

\begin{frame}
\frametitle{Maximum likelihood estimation}
Materials:
\begin{itemize}
    \item Chapter I.5.5 from \cite{deeplearning}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum likelihood estimation}
    \begin{itemize}
        \item We would like to have some principle from which we can derive good estimator functions for a large scale of models.
        \item The {\bf maximum likelihood estimation} is the most common such principle.
        \item Given observation data and a corresponding (statistical) model our goal is to find the parameter vector which imply the highest probability to obtain the data.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Maximum likelihood estimation}
    \begin{itemize}
        \item Consider a set of $m$ examples $\field{X} = \{ \vect{x}^{(1)}, \vect{x}^{(2)}, \ldots,  \vect{x}^{(m)} \}$ drawn independently from the true but unknown distribution $p_{\mbox{data}}(\text{\bf x})$.
        \item Let $p_{\mbox{model}}(\text{\bf x};\vect{\theta})$ be a parametric family of probability distributions, i.e., for each $\vect{\theta}$ we get a different distribution  $p_{\mbox{model}}$.
        \item $p_{\mbox{model}}(\vect{x};\vect{\theta})$ maps any configuration $\vect{x}$ to a real number estimating the (true) probability $p_{\mbox{data}}(\vect{x})$
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Maximum likelihood estimation}
    \begin{itemize}
        \item The maximum likelihood estimator for $\vect{\theta}$ is then defined as
        $$
        \vect{\theta}_{\mbox{ML}} = \mbox{argmax}_{\vect{\theta}} p_{\mbox{model}}(\field{X};\vect{\theta}) =  \mbox{argmax}_{\vect{\theta}} \prod_{i = 1}^{m} p_{\mbox{model}}(\vect{x}^{(i)};\vect{\theta})
        $$
        Note that also the empirical distribution $\hat{p}_{\mbox{data}}$ is implicitly present in the formula through $\vect{x}^{(i)}$.
        \item A more convenient equivalent optimization problem is obtained by taking logarithm of the product
        $$
        \vect{\theta}_{\mbox{ML}} = \mbox{argmax}_{\vect{\theta}} p_{\mbox{model}}(\field{X};\vect{\theta}) = \sum_{i=1}^{m} \log p_{\mbox{model}}\vect{x}^{(i)};\vect{\theta})
        $$
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Maximum likelihood estimation}
    \begin{itemize}
        \item We can further rescale by dividing the expression by $m$
        $$
        \vect{\theta}_{\mbox{ML}} = \mbox{argmax}_{\vect{\theta}} p_{\mbox{model}}(\field{X};\vect{\theta}) = \field{E}_{\text{\bf x} \sim  \hat{p}_{\mbox{data}}} \log p_{\mbox{model}}(\vect{x};\vect{\theta})
        $$
        \item In this way the problem is expressed as an equivalent expectation problem (now the empirical distribution $\hat{p}_{\mbox{data}}$ becomes explicit).
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Maximum likelihood estimation}
    \begin{itemize}
        \item Perhaps more straightforwardly, the maximum likelihood estimation can be seen as minimizing the dissimilarity between $\hat{p}_{\mbox{data}}$ and $p_{\mbox{model}}$.
        \item The degree of dissimilarity is given by the KL-divergence
        $$
        D_{\mbox{KL}} (\hat{p}_{\mbox{data}} \lVert p_{\mbox{model}}) = \field{E}_{\text{\bf x} \sim  \hat{p}_{\mbox{data}}} [\log \hat{p}_{\mbox{data}}(\vect{x}) - \log p_{\mbox{model}}(\vect{x})]
        $$
        \item Only the term of the right is function of the model, so it is the only one which needs to be minimized
        $$
        - \field{E}_{\text{\bf x} \sim  \hat{p}_{\mbox{data}}} [\log p_{\mbox{model}}(\vect{x})]
        $$
        which is equivalent with the maximization problem from the previous slide.
        \item It boils down to minimizing the {\bf cross-entropy} between the two distributions.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Maximum likelihood estimation}
    \begin{itemize}
        \item {\bf The maximum likelihood estimation can be seen as an attempt to make the model distribution } $p_{\mbox{model}}$ {\bf to match the empirical distribution} $\hat{p}_{\mbox{data}}$.
        \item Ideally we would like to match the data generating distribution $p_{\mbox{data}}$, but we do not have access to it.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Conditional log likelihood and mean square error}
    \begin{itemize}
        \item The maximal likelihood estimator can be generalized to estimate a conditional probability $P(\text{\bf y} \mid \text{\bf x} ; \vect{\theta})$ .
        \item Let all inputs be given by $\vect{X}$ and all observed outputs by $\vect{Y}$. Then the conditional maximum likelihood estimator is
        $$
        \vect{\theta_{\mbox{ML}}} = \mbox{arg max}_{\theta} P(\vect{Y} \mid \vect{X}; \vect{\theta})
        $$
        \item If the examples are assumed to be i.i.d., then this can be decomposed into
        $$
        \vect{\theta_{\mbox{ML}}} = \mbox{arg max}_{\theta} \sum_{i=1}^{m} \log P(\vect{y}^{(i)} \mid \vect{x}^{(i)}; \vect{\theta})
        $$
    \end{itemize}
\end{frame}

\iffull
\begin{frame}
\frametitle{Example: linear regression as maximum likelihood}
    \begin{itemize}
        \item The linear regression seen as an algorithm that learns to take an input $\vect{x}$ and produce output $\hat{y}$.
        \item This function from $\vect{x}$ to $\hat{y}$ is chosen to minimize the mean squared error.
        \item This criterion was introduced more or less arbitrarily.
        \item We revisit linear regression from the point of view of maximal likelihood.
        \item We think of the model as producing a conditional distribution $p(y \mid \vect{x})$ instead of a single prediction $\hat{y}$.
    \end{itemize}
\end{frame}
\fi

\begin{frame}
\frametitle{Example: linear regression as maximum likelihood}
    \begin{itemize}
        \item With an infinitely large training set we might see several examples whit the same input $\vect{x}$ but different $y$.
        \item The learning algorithm needs to fit the distribution to all these $y$ corresponding to the same $\vect{x}$.
        \item To derive the linear regression algorithm we assume $p(y \mid \vect{x}) = \mathcal{N}(y; \hat{y}(\vect{x};\vect{w}), \sigma^2)$, where $\hat{y}(\vect{x};\vect{w})$ gives the (prediction of the) mean of the normal distribution and $\sigma$ is fixed to some chose constant.
        \item The parameter vector $\theta$ corresponds in this case to $\vect{w}$.
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Example: linear regression as maximum likelihood}
    \begin{itemize}
        \item By substituting (the full Gaussian function version of) $p(y \mid \vect{x})$ in the conditional log-likelihood formula we obtain
        $$
        \sum_{i=1}^{m} \log p(\vect{x}^{(i)} \mid y^{(i)}; \vect{\theta}) =
        -m \log \sigma - \frac{m}{2} \log(2\pi) - \sum_{i=1}^{m }\frac{\norm{\hat{y}^{(i)} - y^{(i)}}^2}{2 \sigma^2}
        $$
        where $\hat{y}^{(i)}$ is the linear regression on the $i$-th input $\vect{x}^{(i)}$.
        \item By comparing with the mean squared error
        $$
           \mbox{MSE}_{\mbox{train}} = \frac{1}{m} \sum_{i=1}^{m} \norm{\hat{y}^{(i)} - y^{(i)}}^2
        $$
        one can see that maximizing the log-likelihood with respect to $\vect{w}$ results with the same estimate of $\vect{x}$ as minimizing MSE. \iffull {\small (The third term in the log-likelihood forumula, needs to be as small as possible.)} \fi
    \end{itemize}
\end{frame}

\iffull
\begin{frame}
\frametitle{Properties of maximum likelihood}
    \begin{itemize}
        \item It can be shown that the maximum likelihood estimator is the best asymptotically, i.e. as $m \rightarrow \infty$, in terms of its convergence rate.
        \item {\bf Property of consistency}: as the number of training examples approaches infinity the maximum likelihood estimate of a parameter converges towards the true parameter value.
        \item The maximum likelihood estimator has the property of consistency provided:
            \begin{itemize}
                \item The true distribution $p_{\mbox{data}}$ is in the model family
                $p_{\mbox{model}}(\cdot; \vect{\theta})$ %\\
                %(otherwise no estimator can recover $p_{\mbox{data}}$)
                \item $p_{\mbox{data}}$ corresponds to exactly one value of $\vect{\theta}$ \\
                %(Otherwise, although $p_{\mbox{data}}$ might be recovered, it is not possible to dtermine which value of $\thata$ was used in the generate the data)
            \end{itemize}
    \end{itemize}
\end{frame}
\fi

\section{Supervised and unsupervised learning algorithms}

\begin{frame}
\frametitle{Supervised and unsupervised learning algorithms}
Materials:
\begin{itemize}
    \item Chapters I.5.6 and I.5.7 from \cite{deeplearning}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Supervised learning algorithms}
    \begin{itemize}
        \item Learning algorithms that learn based on a given training examples $\vect{x}$ and their corresponding outputs $\vect{y}$.
        \begin{itemize}
            \item Linear and logistic regressions
            \item Support vector machines
            \item $k$-nearest neighbours
            \item Decision trees
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Unsupervised learning algorithms}
    \begin{itemize}
        \item Unsupervised algorithms experience only "features", but not supervision feedback.
        \item The distinction with the supervised algorithms is not always clear since there is no good test to distinguish if something is a feature or a target provided by the supervisor.
        \item Rule of thumb: in unsupervised algorithms no human annotation is needed for the training examples.
        \begin{itemize}
            \item Principal component analysis
            \item $k$-means clustering
            \item t-Distributed Stochastic Neighbor Embedding
            \item Generative adversarial networks
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Model evaluation}

\begin{frame}
\frametitle{Model evaluation}
Materials:
\begin{itemize}
    \item \cite{ROC}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Model evaluation}
    \begin{itemize}
        \item To quantitatively evaluate a machine learning algorithm we need to define a {\bf performance measure}.
        \item Usually the performance measure is specific to the task carried out by the algorithm.
        \item For classification tasks a natural measure is the model {\bf accuracy}.
        \item The {\bf accuracy} is defined as the proportion of examples for which the model produces the correct output.
        \item An equivalent (complementary) measure is the {\bf error rate} defined as the proportion of incorrect outputs.
    \end{itemize}
\end{frame}

\begin{frame}{Model evaluation}
    \begin{itemize}
        \item The best way to evaluate a machine learning algorithm is by applying it to a {\bf test set} data which has not been seen before.
        \item Ideally there should be {\bf no overlap} between the {\bf test set} and  the {\bf training set} used to obtain the model.
    \end{itemize}
\end{frame}

\begin{frame}\frametitle{Binary classification}
    \begin{itemize}
        \item We consider {\bf binary classification} problems, i.e., problems using only two classes/
        \item Formally each input example $x^{(i)}$ needs to be mapped into one element of the set $\{\vect{p},\vect{n}\}$ of {\bf true} classes.
        \item A {\bf classification model} ({\bf classifier}) is a function from the input examples to  from the set $\{\vect{Y}, \vect{N}\}$ of {\bf predicted classes} or {\bf hypothesized classes}.
        \item $\vect{p},\vect{n}$ correspond to $\vect{Y},\vect{N}$, respectively.
    \end{itemize}
\end{frame}


\begin{frame}\frametitle{Binary classification}
    \begin{itemize}
        \item For a given classifier there are four possible outcomes.
        \item If the true class of $x^{(i)}$ is $\vect{p}$ and the predicted class is $\vect{Y}$ then we have a {\bf true positive} (TP); if it was classified $\vect{N}$, then we have a {\bf false negative} (FN).
        \item Symmetrically, a  $x^{(i)}$ with true class $\vect{n}$ which is assigned a predicted class $\vect{N}$ is a {\bf true negative} (TN); if the predicted class is $\vect{Y}$, then it is a false positive (FP).
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Confusion matrix}
     These four combinations can be put together in a {\bf confusion matrix}, also called {\bf contingency table}.
     \begin{table}[Confusion Matrix]
            \begin{tabular}{c|c|c|c|}
            \multicolumn{2}{c}{} & \multicolumn{2}{c}{True class} \\
            \cline{3-4}
            \multicolumn{2}{c|}{} & $\vect{p}$ & $\vect{n}$ \\
            \cline{2-4}
            \multirow{6}{*}{Predicted class} & \multirow{3}{*}{$\vect{Y}$} &  True & False  \\
                &   & positives & positives  \\
                &   & (TP) & (FP) \\
            \cline{2-4}
                & \multirow{3}{*}{$\vect{N}$} & False &  True  \\
                &   &  negatives & negatives  \\
                &   & (FN) & (TN) \\
            \cline{2-4}
            \end{tabular}
        \end{table}
\end{frame}



\begin{frame}
 \frametitle{Binary classifications metrics}
    \begin{itemize}
        \item Using the four basic categories of prediction outcomes (TP, FP, TN, FN) we can derive various {\bf  measures} of performance of classification models.
        \item For instance the accuracy can be defined as
        $$\mbox{Accuracy} = \frac{TP+TN}{TP+FP+TN+FN}$$
        \item Also quite frequently used measures are \\

    \end{itemize}
    \small{
        $$Sensitivity = \frac{TP}{TP+FN} \hspace{1em} Specificity = \frac{TN}{TN+FP} \hspace{1em} Precision = \frac{TP}{TP+FP}$$
        }
\end{frame}


\begin{frame}
\frametitle{Binary classification metrics}
    \begin{itemize}
        \item Sensitivity is also called {\bf recall}, {\bf true positive rate} or {\bf hit rate}
        \item In medical contexts the sensitivity can be interpreted as a measure of the extent to which diseased individuals are incorrectly diagnosed.
        \item In general: measures the proportion of the target group the method is able to detect, i.e. how sensitive is to this group.
        \item Specificity is also called {\bf true negative rate} or {\bf selectivity}.
        \item In medical contexts the specificity can be interpreted as a measure of the extent to which healthy individuals are correctly diagnosed.
        \item The precision tells us which proportion of the positive predictions is correct.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Binary classification metrics}
    \begin{itemize}
        \item Sometimes the above mentioned measures are not sufficient.
        \item For example, in a population in which the percentage of healthy individuals is much larger than the diseased individuals, it is easy to achieve high specificity by trivially classifying each patient as healthy.
        \item We can obtain more objective evaluation by combing metrics.
        \item The metrics $F_1$ is the harmonic mean (average) of the precision and recall (sensitivity)
        $$
        \frac{2}{F_1} = \frac{1}{Precision}+\frac{1}{Recall} \text{  or  } F_1 = 2 \cdot \frac{Precision \cdot Recall}{Precision+Recall}
        $$
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Areas under the curve measures}
    \begin{itemize}
        \item (Binary) classifications often depend on some parameter (e.g., threshold).
        \item Hence on way to combine two metrics is by assigning them to the axes of a coordinate system and varying this parameter to construct a graphical plot.
        \item We obtain a curve (actually, most of the time series of points) such that each point corresponds to a particular parameter value.
        \item The area under the curve is a measure of how good is the classification.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Receiver Operating Characteristic (ROC) curve}
    \begin{itemize}
        \item The ROC curve plots the true positive rate (sensitivity) versus the false positive rate (1 - specificity).
        \begin{center}
        \includegraphics[width=0.95\textwidth]{figures/week_2/camelyon_roc.png}
        \end{center}
    \end{itemize}
    {\tiny Figure from \cite{camelyon}}
\end{frame}

\begin{frame}
\frametitle{Receiver Operating Characteristic (ROC) curve}
    \begin{itemize}
        \item  There are several characteristic points points in the ROC space:
            \begin{itemize}
                \item (0,0) corresponds to the strategy of never making a positive classification.
                \item (1,1) is the opposite: unconditionally issuing a positive classification.
                \item (0,1) represents perfect classification.
                \item Obviously we strive to achieve this ideal point as as a result have as much as possible area under the curve covered \\
                (ideally it should cover the whole square corresponding to the ROC space)
            \end{itemize}
        \item  A less common example of a measure combination into a graphical plot is the precision-recall plot (recall on the $x$-axis, precision on the $y$-axis).
    \end{itemize}
\end{frame}


\section{Ensambling}

\begin{frame}
\frametitle{Ensambling}
Materials:
\begin{itemize}
    \item Chapter II.7.11 from \cite{deeplearning}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bagging and other ensemble methods}
    \begin{itemize}
        \item {\bf Bagging} (short for {\bf b}ootstrap {\bf agg}regat{\bf ing}) is a technique for reducing of the generalization error by combining several models.
        \item Train models separately and let them vote on the right output.
        \item An example of a general strategy in machine learning called {\bf model averaging}.
        \item Methods using this strategy are called {\em ensemble methods}.
        \item The rationale behind the combining of models is that usually different models will not make the same error.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bagging}
    \begin{itemize}
        \item Different ensemble methods compose the ensemble of models in different ways.
        \item One way would be to choose the models, training algorithms and objective functions as different as possible.
        \item In contrast, begging allows the same kind of model, algorithm and objective function to be reused several times.
    \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Bagging}
    \begin{itemize}
        \item Bagging constructs $k$ different data sets.
        \item Each data set
        \begin{itemize}
            \item has the same size as the original set and
            \item is constructed by sampling with repetition from the original data set
        \end{itemize}
        \item For each data set a different model is produced.
        \item Each model reflects the differences between the (training) data sets.
    \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Bagging example}
    Training an ``8 detector'' with two resampled datasets: a ``cartoon'' example.
    \begin{center}
        \includegraphics[width=0.75\textwidth]{figures/week_2/bagging_cartoon.png}
    \end{center}
    \tiny{Figure from \cite{deeplearning}}
\end{frame}

\begin{frame}
\frametitle{Model averaging for neural networks}
    \begin{itemize}
        \item Neural networks profit from model averaging even when they are trained on the same data set.
        \item This is because with random initialization, minibatches (subsets of the training set), hyperparameters, non-determinism in the implementation a sufficient variety between the models can be achieved.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Model averaging in general}
    \begin{itemize}
        \item In general, it is considered that model averaging always improves the generalization error.
        \item In theory, with sufficient computer memory and time one can always improve the results by combining several methods.
        \item Therefore, when testing/benchmarking (new) methods it is considered "fair" to use only a single model.
        \item Machine learning contests are usually won by using model averaging.
        \item {\bf Boosting} is similar to ensembling, only the models (neural networks) are added {\bf incrementally} to the ensemble.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Acknowledgements}

The slides for this lecture were prepared by Mitko Veta and Dragan Bo{\v s}nacki.

\end{frame}


\begin{frame}
\frametitle{References}
\printbibliography
\end{frame}

\end{document}
